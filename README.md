# Data-science-project2
In this project, I implemented image classification using linear SVM based on deep learning without feature dimensionality reduction and using three different feature dimensionality reduction methods using AwA2 dataset, and compared the performance of model training and testing under different feature dimensions and different dimensionality reduction methods. 

AwA2 dataset consists of 37322 images of 50 animals’ classes with pre-extracted feature representations for each image. In this project, I used the image features extracted from the pre trained resnet101 model to feed into the model. These feature vectors are provided on the AwA2 dataset website and stored in the “AwA2-features.txt” file, along with the corresponding label file “AwA2-labels.txt”.

In order to complete the three tasks, I implemented three Python files to implement different functions. The '*k\_fold.py*' file determined the optimal k-value of the KNN algorithm through k-fold cross validation when the distance metrics were Euclidean distance and Manhattan distance, respectively. The '*main.py*' file obtained the final model testing accuracy when the distance metrics were Euclidean distance and Manhattan distance, respectively, when the optimal k-value was selected for KNN. The '*metric\_learning.py*' added the LMNN algorithm on top of '*main.py*' to improve the performance of KNN.

In terms of data processing, the above three files were all cut into training and testing sets in a ratio of six to four after reading the features and labels data. At the same time, PCA dimensionality reduction was performed on the feature data, reducing the feature dimension to 300 dimensions. In the '*metric\_learning.py*' file, due to calling the LMNN function, if no other preprocessing is performed on the dataset, it will result in excessive memory usage (approximately 285GB). Therefore, before cutting the dataset, the size of the dataset will be randomly reduced to 0.3 times the original size.

By running '*k\_fold.py*', the optimal k-value of the KNN algorithm was determined to be **6**, and this k-value was applied to '*main.py*' and '*metric\_learning.py*'. After running the '*main.py*' file, the final model testing accuracy of the KNN algorithm was **0.896** and **0.866** for distance metrics of Euclidean distance and Manhattan distance, respectively. **It can be seen that setting distance metrics to Euclidean distance can achieve better results.**

Based on this, set the distance metrics to Euclidean distance in "*metric learning.py*". After 1000 iterations of the LMNN algorithm, the accuracy of the final model reached **0.907**. Compared with the previous **0.896**, it can be found that to some extent, LMNN did improve the performance of the KNN algorithm.

